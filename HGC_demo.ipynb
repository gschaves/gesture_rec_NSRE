{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1127916,"status":"ok","timestamp":1719411589047,"user":{"displayName":"Gabriel Chaves","userId":"05552850817445874347"},"user_tz":180},"id":"njv6Mc3eyM94","outputId":"48cfebd7-9ff1-4d49-98d0-d2fadfd3e42c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-d99178a0-a71d-4e43-a4ce-2f197b48da05\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d99178a0-a71d-4e43-a4ce-2f197b48da05\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving alejandroFlores_double_tap.csv to alejandroFlores_double_tap (1).csv\n","Saving alejandroFlores_fingers_spread.csv to alejandroFlores_fingers_spread (1).csv\n","Saving alejandroFlores_fist.csv to alejandroFlores_fist (1).csv\n","Saving alejandroFlores_wave_in.csv to alejandroFlores_wave_in (1).csv\n","Saving alejandroFlores_wave_out.csv to alejandroFlores_wave_out (1).csv\n","Saving alexandraApellido_double_tap.csv to alexandraApellido_double_tap (1).csv\n","Saving alexandraApellido_fingers_spread.csv to alexandraApellido_fingers_spread (1).csv\n","Saving alexandraApellido_fist.csv to alexandraApellido_fist (1).csv\n","Saving alexandraApellido_wave_in.csv to alexandraApellido_wave_in (1).csv\n","Saving alexandraApellido_wave_out.csv to alexandraApellido_wave_out (1).csv\n","Saving andresGuerra_double_tap.csv to andresGuerra_double_tap (1).csv\n","Saving andresGuerra_fingers_spread.csv to andresGuerra_fingers_spread (1).csv\n","Saving andresGuerra_fist.csv to andresGuerra_fist (1).csv\n","Saving andresGuerra_wave_in.csv to andresGuerra_wave_in (1).csv\n","Saving andresGuerra_wave_out.csv to andresGuerra_wave_out (1).csv\n","Saving andresJaramillo_double_tap.csv to andresJaramillo_double_tap (1).csv\n","Saving andresJaramillo_fingers_spread.csv to andresJaramillo_fingers_spread (1).csv\n","Saving andresJaramillo_fist.csv to andresJaramillo_fist (1).csv\n","Saving andresJaramillo_wave_in.csv to andresJaramillo_wave_in (1).csv\n","Saving andresJaramillo_wave_out.csv to andresJaramillo_wave_out (1).csv\n","Saving cristhianMotoche_double_tap.csv to cristhianMotoche_double_tap (1).csv\n","Saving cristhianMotoche_fingers_spread.csv to cristhianMotoche_fingers_spread (1).csv\n","Saving cristhianMotoche_fist.csv to cristhianMotoche_fist (1).csv\n","Saving cristhianMotoche_wave_in.csv to cristhianMotoche_wave_in (1).csv\n","Saving cristhianMotoche_wave_out.csv to cristhianMotoche_wave_out (1).csv\n","Saving dianitaCherrez_double_tap.csv to dianitaCherrez_double_tap (1).csv\n","Saving dianitaCherrez_fingers_spread.csv to dianitaCherrez_fingers_spread (1).csv\n","Saving dianitaCherrez_fist.csv to dianitaCherrez_fist (1).csv\n","Saving dianitaCherrez_wave_in.csv to dianitaCherrez_wave_in (1).csv\n","Saving dianitaCherrez_wave_out.csv to dianitaCherrez_wave_out (1).csv\n","Saving homeroApellido_double_tap.csv to homeroApellido_double_tap (1).csv\n","Saving homeroApellido_fingers_spread.csv to homeroApellido_fingers_spread (1).csv\n","Saving homeroApellido_fist.csv to homeroApellido_fist (1).csv\n","Saving homeroApellido_wave_in.csv to homeroApellido_wave_in (1).csv\n","Saving homeroApellido_wave_out.csv to homeroApellido_wave_out (1).csv\n","Saving jonathanZea_double_tap.csv to jonathanZea_double_tap (1).csv\n","Saving jonathanZea_fingers_spread.csv to jonathanZea_fingers_spread (1).csv\n","Saving jonathanZea_fist.csv to jonathanZea_fist (1).csv\n","Saving jonathanZea_wave_in.csv to jonathanZea_wave_in (1).csv\n","Saving jonathanZea_wave_out.csv to jonathanZea_wave_out (1).csv\n","Saving juanYuquilema_double_tap.csv to juanYuquilema_double_tap (1).csv\n","Saving juanYuquilema_fingers_spread.csv to juanYuquilema_fingers_spread (1).csv\n","Saving juanYuquilema_fist.csv to juanYuquilema_fist (1).csv\n","Saving juanYuquilema_wave_in.csv to juanYuquilema_wave_in (1).csv\n","Saving juanYuquilema_wave_out.csv to juanYuquilema_wave_out (1).csv\n","Saving santiagoJaramillo_double_tap.csv to santiagoJaramillo_double_tap (1).csv\n","Saving santiagoJaramillo_fingers_spread.csv to santiagoJaramillo_fingers_spread (1).csv\n","Saving santiagoJaramillo_fist.csv to santiagoJaramillo_fist (1).csv\n","Saving santiagoJaramillo_wave_in.csv to santiagoJaramillo_wave_in (1).csv\n","Saving santiagoJaramillo_wave_out.csv to santiagoJaramillo_wave_out (1).csv\n","User uploaded file \"alejandroFlores_double_tap (1).csv\" with length 2498402 bytes\n","User uploaded file \"alejandroFlores_fingers_spread (1).csv\" with length 2497807 bytes\n","User uploaded file \"alejandroFlores_fist (1).csv\" with length 2493243 bytes\n","User uploaded file \"alejandroFlores_wave_in (1).csv\" with length 2498034 bytes\n","User uploaded file \"alejandroFlores_wave_out (1).csv\" with length 2496673 bytes\n","User uploaded file \"alexandraApellido_double_tap (1).csv\" with length 2497983 bytes\n","User uploaded file \"alexandraApellido_fingers_spread (1).csv\" with length 2494859 bytes\n","User uploaded file \"alexandraApellido_fist (1).csv\" with length 2495135 bytes\n","User uploaded file \"alexandraApellido_wave_in (1).csv\" with length 2494230 bytes\n","User uploaded file \"alexandraApellido_wave_out (1).csv\" with length 2497885 bytes\n","User uploaded file \"andresGuerra_double_tap (1).csv\" with length 2499973 bytes\n","User uploaded file \"andresGuerra_fingers_spread (1).csv\" with length 2497866 bytes\n","User uploaded file \"andresGuerra_fist (1).csv\" with length 2493724 bytes\n","User uploaded file \"andresGuerra_wave_in (1).csv\" with length 2492628 bytes\n","User uploaded file \"andresGuerra_wave_out (1).csv\" with length 2492874 bytes\n","User uploaded file \"andresJaramillo_double_tap (1).csv\" with length 2490679 bytes\n","User uploaded file \"andresJaramillo_fingers_spread (1).csv\" with length 2491897 bytes\n","User uploaded file \"andresJaramillo_fist (1).csv\" with length 2490359 bytes\n","User uploaded file \"andresJaramillo_wave_in (1).csv\" with length 2491434 bytes\n","User uploaded file \"andresJaramillo_wave_out (1).csv\" with length 2490205 bytes\n","User uploaded file \"cristhianMotoche_double_tap (1).csv\" with length 2492611 bytes\n","User uploaded file \"cristhianMotoche_fingers_spread (1).csv\" with length 2491913 bytes\n","User uploaded file \"cristhianMotoche_fist (1).csv\" with length 2491581 bytes\n","User uploaded file \"cristhianMotoche_wave_in (1).csv\" with length 2492370 bytes\n","User uploaded file \"cristhianMotoche_wave_out (1).csv\" with length 2490403 bytes\n","User uploaded file \"dianitaCherrez_double_tap (1).csv\" with length 2490015 bytes\n","User uploaded file \"dianitaCherrez_fingers_spread (1).csv\" with length 2495397 bytes\n","User uploaded file \"dianitaCherrez_fist (1).csv\" with length 2491579 bytes\n","User uploaded file \"dianitaCherrez_wave_in (1).csv\" with length 2491389 bytes\n","User uploaded file \"dianitaCherrez_wave_out (1).csv\" with length 2493162 bytes\n","User uploaded file \"homeroApellido_double_tap (1).csv\" with length 2497163 bytes\n","User uploaded file \"homeroApellido_fingers_spread (1).csv\" with length 2496155 bytes\n","User uploaded file \"homeroApellido_fist (1).csv\" with length 2495590 bytes\n","User uploaded file \"homeroApellido_wave_in (1).csv\" with length 2496855 bytes\n","User uploaded file \"homeroApellido_wave_out (1).csv\" with length 2492170 bytes\n","User uploaded file \"jonathanZea_double_tap (1).csv\" with length 2004547 bytes\n","User uploaded file \"jonathanZea_fingers_spread (1).csv\" with length 2006876 bytes\n","User uploaded file \"jonathanZea_fist (1).csv\" with length 2005549 bytes\n","User uploaded file \"jonathanZea_wave_in (1).csv\" with length 2028552 bytes\n","User uploaded file \"jonathanZea_wave_out (1).csv\" with length 1986623 bytes\n","User uploaded file \"juanYuquilema_double_tap (1).csv\" with length 2496763 bytes\n","User uploaded file \"juanYuquilema_fingers_spread (1).csv\" with length 2496505 bytes\n","User uploaded file \"juanYuquilema_fist (1).csv\" with length 2494168 bytes\n","User uploaded file \"juanYuquilema_wave_in (1).csv\" with length 2494251 bytes\n","User uploaded file \"juanYuquilema_wave_out (1).csv\" with length 2492812 bytes\n","User uploaded file \"santiagoJaramillo_double_tap (1).csv\" with length 2079981 bytes\n","User uploaded file \"santiagoJaramillo_fingers_spread (1).csv\" with length 2067140 bytes\n","User uploaded file \"santiagoJaramillo_fist (1).csv\" with length 2104785 bytes\n","User uploaded file \"santiagoJaramillo_wave_in (1).csv\" with length 2012283 bytes\n","User uploaded file \"santiagoJaramillo_wave_out (1).csv\" with length 2049196 bytes\n"]}],"source":["#Importing data from local computer\n","\n","from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31240,"status":"ok","timestamp":1719411900956,"user":{"displayName":"Gabriel Chaves","userId":"05552850817445874347"},"user_tz":180},"id":"Ka1OZ4Nj51L5","outputId":"62b8e9ce-8185-43e1-ed50-74f2ee44e947"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fastdtw\n","  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/133.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m133.1/133.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastdtw) (1.25.2)\n","Building wheels for collected packages: fastdtw\n","  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=512616 sha256=ef850885693dc8718565acc2673f1cefe528bff8a397a908c5dc4bfb51eb6bc8\n","  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\n","Successfully built fastdtw\n","Installing collected packages: fastdtw\n","Successfully installed fastdtw-0.3.4\n","Collecting dtaidistance\n","  Downloading dtaidistance-2.3.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dtaidistance) (1.25.2)\n","Installing collected packages: dtaidistance\n","Successfully installed dtaidistance-2.3.12\n"]}],"source":["!pip install fastdtw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vug5Al_ux4h_"},"outputs":[],"source":["# aux_fnc\n","\n","from scipy import signal\n","import numpy as np\n","import pandas as pd\n","from fastdtw import fastdtw\n","from scipy.spatial.distance import euclidean\n","\n","def trial(data,flag_index,i_trial):\n","\n","  data_trial = data[flag_index[i_trial]+1:flag_index[i_trial+1]]\n","\n","  return data_trial\n","\n","#Function that rectifies and filters the signal\n","def filt_emg(emgs_raw,b,a):\n","\n","  emg_filt = []\n","\n","  for emg_raw in emgs_raw:\n","    #Signal normalized\n","    if np.max(abs(emg_raw))>1:\n","      emg_norm = emg_raw/128\n","    else:\n","      emg_norm = emg_raw\n","\n","    #Signal rectified (R)\n","    emg_rect = abs(emg_norm)\n","\n","    #Signal filtered (V)\n","    emg_filt.append(signal.filtfilt(b,a,emg_rect,axis=0))\n","\n","  return emg_filt\n","################################################################################\n","\n","#Function that detect the muscle activity\n","def muscle_activity(emgs_filt,fs,window,numFreqOfSpec,nOver_win,hamm_win,tau_u,min_seg):\n","\n","  emgs_ts         = []\n","  gesture_index_s = []\n","  gesture_index_e = []\n","\n","  tau_u_aux   = 0\n","  tau_u_count = 0\n","  for emg_filt in emgs_filt:\n","    #Sum along the rows (S)\n","    emg_sum = np.sum(emg_filt,axis=-1)\n","\n","    #Spectrogram (P_C)\n","    f,t,spec = signal.stft(emg_sum,fs=fs,window=window,nperseg=numFreqOfSpec,noverlap=nOver_win,nfft=numFreqOfSpec*2,boundary=None, padded=False)\n","\n","    #Modulus (P)\n","    spec_abs = ((hamm_win+1)/2)*abs(spec)\n","\n","    #Sum along the rows (U)\n","    spec_sum = np.sum(spec_abs,0)\n","\n","    # print(spec_sum)\n","    # input(\"Press Enter\")\n","\n","    #Muscle activity detection\n","    great_than_tau = spec_sum >= tau_u\n","    great_than_tau = np.concatenate([[0],great_than_tau.astype(int),[0]])\n","\n","    diff_great_than_tau = abs(np.diff(great_than_tau))\n","\n","    if diff_great_than_tau[-1]==1:\n","      diff_great_than_tau[-2]=1\n","\n","    diff_great_than_tau= diff_great_than_tau[:-1]\n","\n","    index_nonzero = np.where(diff_great_than_tau==1)\n","    index_nonzero = np.array(index_nonzero[0])\n","\n","    index_samp = np.floor(t*fs)-1\n","\n","    nindex_nonzero = len(index_nonzero)\n","\n","    #Where the muscle activity starts and ends\n","    #First condition: none activity is detected\n","    #Second condition: only the start is detected\n","    #Third condition: the start and end is detected\n","    if nindex_nonzero==0:\n","      index_s = 0\n","      index_e = len(emg_sum)\n","    elif nindex_nonzero==1:\n","      index_s = index_samp[index_nonzero].astype(int)\n","      index_e = len(emg_sum)\n","    else:\n","      index_s = index_samp[index_nonzero[0]].astype(int)\n","      index_e = index_samp[index_nonzero[-1]-1].astype(int)+1\n","    #end\n","\n","    nxtra_samples = 25;\n","    index_s = np.maximum(0, index_s - nxtra_samples)\n","    index_e = np.minimum(len(emg_sum), index_e + nxtra_samples)\n","\n","    #Check if the length of the activity is greater than \"min_seg\"\n","    #Else, all the signal is selected\n","    if (index_e - index_s) < min_seg:\n","      index_s = 0\n","      index_e = len(emg_sum)\n","    #end\n","\n","    gesture_index_s = np.append(gesture_index_s,index_s)\n","    gesture_index_e = np.append(gesture_index_e,index_e)\n","    emgs_ts.append(emg_filt[index_s:index_e,:])\n","    # plt.plot(emg_filt[index_s:index_e,:])\n","    # plt.show()\n","    # input(\"Press Enter to continue\")\n","  #end\n","\n","  return emgs_ts, spec, spec_abs, gesture_index_s.astype(int), gesture_index_e.astype(int)\n","################################################################################\n","\n","#Compute the best centers of the class\n","def best_center_class(emgs_ts):\n","  mtxDistances = np.zeros((len(emgs_ts), len(emgs_ts)))\n","\n","  for j in range(len(emgs_ts)):\n","    for k in range(len(emgs_ts)):\n","      if k>j:\n","        dist_dtw, path_dtw = fastdtw(emgs_ts[j],emgs_ts[k],radius = 100,dist=2)\n","        mtxDistances[j,k] = dist_dtw\n","        mtxDistances[k,j] = dist_dtw\n","      #end\n","\n","    #end\n","\n","  #end\n","\n","  vectDistances = np.sum(mtxDistances,0)\n","  idx_min       = np.argmin(vectDistances)\n","  center_idx    = emgs_ts[idx_min]\n","  # print(mtxDistances,vectDistances,idx_min,center_idx, sep='\\n')\n","\n","  return center_idx\n","################################################################################\n","\n","#Compute the best centers of the class\n","def best_center_dtai(emgs_ts):\n","  mtxDistances = np.zeros((len(emgs_ts), len(emgs_ts)))\n","\n","  for j in range(len(emgs_ts)):\n","    for k in range(len(emgs_ts)):\n","      if k>j:\n","        dist_dtw = dtw.distance_fast(emgs_ts[j],emgs_ts[k],use_ndim=True,use_c=True)\n","        mtxDistances[j,k] = dist_dtw\n","        mtxDistances[k,j] = dist_dtw\n","      #end\n","\n","    #end\n","\n","  #end\n","\n","  vectDistances = np.sum(mtxDistances,0)\n","  idx_min       = np.argmin(vectDistances)\n","  center_idx    = emgs_ts[idx_min]\n","  # print(mtxDistances,vectDistances,idx_min,center_idx, sep='\\n')\n","\n","  return center_idx\n","################################################################################\n","\n","#Feature extraction\n","def feat_extrac(emgs_ts,center):\n","  dataX_temp   = [fastdtw(emg_ts,center,radius = 100,dist=2) for emg_ts in emgs_ts]\n","  dataX, pathX = map(list,zip(*dataX_temp))\n","\n","  return dataX\n","################################################################################\n","\n","# DTW feature extraction\n","def dtw_extract(emgs_ts,center):\n","  dataX_temp   = [fastdtw(emg_ts,center,radius = 100,dist=2) for emg_ts in emgs_ts]\n","  dataX, pathX = map(list,zip(*dataX_temp))\n","\n","  return dataX\n","################################################################################\n","\n","# Dtai DTW extraction\n","def dtaidtw_extract(emgs_ts, center):\n","  dataX_temp = [dtw.distance_fast(emg_ts, center, use_ndim=True, use_c=True) for emg_ts in emgs_ts]\n","\n","  return dataX_temp\n","################################################################################\n","\n","#Feature normalization\n","def feat_norm(dataX):\n","  df_dataX      = dataX.copy()\n","  df_dataX_mean = pd.DataFrame(df_dataX.mean(axis=1),columns=['Mean'])\n","  df_dataX_std  = pd.DataFrame(df_dataX.std(axis=1),columns=['Std'])\n","\n","\n","  normalized_feat = df_dataX.sub(df_dataX_mean['Mean'],axis=0)\n","  normalized_feat = normalized_feat.div(df_dataX_std['Std'],axis=0)\n","\n","  return normalized_feat\n","################################################################################\n","\n","# Feature normalization\n","def feat_normalization(data):\n","  row_means = np.mean(data, axis=1, keepdims=True)\n","  row_stds  = np.std(data, axis=1, keepdims=True, ddof=1)\n","\n","  data_normalized = (data - row_means) / row_stds\n","\n","  return data_normalized\n","################################################################################\n","\n","#Normalized Least Mean Square NLMS\n","def nlms(d,x,mu,N,init,gamma):\n","  nCoefficients = N+1\n","  nIterations   = np.size(d,1)\n","\n","  errVector         = np.zeros((nIterations, 1),dtype='float64')\n","  outputVector      = np.zeros((nIterations, 1),dtype='float64')\n","  coefficientVector = np.zeros((nCoefficients, nIterations+1),dtype='float64')\n","\n","  coefficientVector[:,0] = init.flatten()\n","\n","  prefixedInput = np.vstack(((np.zeros((nCoefficients-1,1),dtype='float64')),np.reshape(x,(len(x),1))))\n","  # print(nCoefficients)\n","  # print(nIterations)\n","  # print(np.shape(prefixedInput))\n","  for it in range(nIterations):\n","\n","    regressor = np.flipud(prefixedInput[it:it + (nCoefficients-1)+1])\n","    # regressor = prefixedInput[-(it + (nCoefficients-1)+1):it:-1]\n","\n","    outputVector[it] = np.dot(coefficientVector[:,it].T,regressor)\n","    # print(np.shape(d))\n","    errVector[it] = d[:,it]-outputVector[it]\n","    # print(errVector[it])\n","\n","    # if it <= 400:\n","    #   coefficientVector[:,it+1] = coefficientVector[:,it]+((mu/(gamma + np.dot(regressor.T,regressor)))*np.conj(errVector[it])*regressor).flatten()\n","    # else:\n","    #   coefficientVector[:,it+1] = coefficientVector[:,it]\n","    coefficientVector[:,it+1] = coefficientVector[:,it]+((mu/(gamma + np.dot(regressor.T,regressor)))*np.conj(errVector[it])*regressor).flatten()\n","\n","  return outputVector, errVector, coefficientVector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31eNCnHiyCHu"},"outputs":[],"source":["#neural_net\n","import numpy as np\n","from scipy.spatial.distance import euclidean\n","\n","# Initialize parameters\n","def initialize_parameters(layers_dims):\n","  \"\"\"\n","  Initialize parameters dictionary.\n","\n","  Weight matrices will be initialized to random values from uniform normal\n","  distribution.\n","  bias vectors will be initialized to zeros.\n","\n","  Arguments\n","  ---------\n","  layers_dims : list or array-like\n","      dimensions of each layer in the network.\n","\n","  Returns\n","  -------\n","  parameters : dict\n","      weight matrix and the bias vector for each layer.\n","  \"\"\"\n","  # np.random.seed(1)\n","  parameters = {}\n","  L = len(layers_dims)\n","\n","  for l in range(1, L):\n","    r = np.sqrt(6) / np.sqrt(layers_dims[l] + layers_dims[l - 1] + 1)\n","    W = np.random.randn(layers_dims[l], layers_dims[l - 1])*2*r - r\n","    parameters[\"W\" + str(l)] = W\n","    parameters[\"b\" + str(l)] = np.ones((layers_dims[l], 1))\n","\n","    assert parameters[\"W\" + str(l)].shape == (layers_dims[l], layers_dims[l - 1]), parameters[\"W\" + str(l)].shape\n","    # assert parameters[\"b\" + str(l)].shape == (layers_dims[l], 1)\n","\n","  return parameters\n","\n","# Define activation functions that will be used in forward propagation\n","def sigmoid(Z):\n","  \"\"\"\n","  Computes the sigmoid of Z element-wise.\n","\n","  Arguments\n","  ---------\n","  Z : array\n","      output of affine transformation.\n","\n","  Returns\n","  -------\n","  A : array\n","      post activation output.\n","  Z : array\n","      output of affine transformation.\n","  \"\"\"\n","  A = 1 / (1 + np.exp(-Z))\n","\n","  return A, Z\n","\n","\n","def tanh(Z):\n","  \"\"\"\n","  Computes the Hyperbolic Tagent of Z elemnet-wise.\n","\n","  Arguments\n","  ---------\n","  Z : array\n","      output of affine transformation.\n","\n","  Returns\n","  -------\n","  A : array\n","      post activation output.\n","  Z : array\n","      output of affine transformation.\n","  \"\"\"\n","  A = np.tanh(Z)\n","\n","  return A, Z\n","\n","\n","def relu(Z):\n","  \"\"\"\n","  Computes the Rectified Linear Unit (ReLU) element-wise.\n","\n","  Arguments\n","  ---------\n","  Z : array\n","      output of affine transformation.\n","\n","  Returns\n","  -------\n","  A : array\n","      post activation output.\n","  Z : array\n","      output of affine transformation.\n","  \"\"\"\n","  A = np.maximum(0, Z)\n","\n","  return A, Z\n","\n","\n","def leaky_relu(Z):\n","  \"\"\"\n","  Computes Leaky Rectified Linear Unit element-wise.\n","\n","  Arguments\n","  ---------\n","  Z : array\n","      output of affine transformation.\n","\n","  Returns\n","  -------\n","  A : array\n","      post activation output.\n","  Z : array\n","      output of affine transformation.\n","  \"\"\"\n","  A = np.maximum(0.1 * Z, Z)\n","\n","  return A, Z\n","\n","# Define helper functions that will be used in L-model forward prop\n","def linear_forward(A_prev, W, b):\n","  \"\"\"\n","  Computes affine transformation of the input.\n","\n","  Arguments\n","  ---------\n","  A_prev : 2d-array\n","      activations output from previous layer.\n","  W : 2d-array\n","      weight matrix, shape: size of current layer x size of previuos layer.\n","  b : 2d-array\n","      bias vector, shape: size of current layer x 1.\n","\n","  Returns\n","  -------\n","  Z : 2d-array\n","      affine transformation output.\n","  cache : tuple\n","      stores A_prev, W, b to be used in backpropagation.\n","  \"\"\"\n","  Z = np.dot(W, A_prev) + b\n","  cache = (A_prev, W, b)\n","\n","  return Z, cache\n","\n","\n","def linear_activation_forward(A_prev, W, b, activation_fn):\n","  \"\"\"\n","  Computes post-activation output using non-linear activation function.\n","\n","  Arguments\n","  ---------\n","  A_prev : 2d-array\n","      activations output from previous layer.\n","  W : 2d-array\n","      weight matrix, shape: size of current layer x size of previuos layer.\n","  b : 2d-array\n","      bias vector, shape: size of current layer x 1.\n","  activation_fn : str\n","      non-linear activation function to be used: \"sigmoid\", \"tanh\", \"relu\".\n","\n","  Returns\n","  -------\n","  A : 2d-array\n","      output of the activation function.\n","  cache : tuple\n","      stores linear_cache and activation_cache. ((A_prev, W, b), Z) to be used in backpropagation.\n","  \"\"\"\n","  # assert activation_fn == \"sigmoid\" or activation_fn == \"tanh\" or \\\n","  #     activation_fn == \"relu\"\n","\n","  if activation_fn == \"sigmoid\":\n","      Z, linear_cache = linear_forward(A_prev, W, b)\n","      A, activation_cache = sigmoid(Z)\n","\n","  elif activation_fn == \"tanh\":\n","      Z, linear_cache = linear_forward(A_prev, W, b)\n","      A, activation_cache = tanh(Z)\n","\n","  elif activation_fn == \"relu\":\n","      Z, linear_cache = linear_forward(A_prev, W, b)\n","      A, activation_cache = relu(Z)\n","\n","  elif activation_fn == \"softmax\":\n","      Z, linear_cache = linear_forward(A_prev, W, b)\n","      Zaux = Z - np.amax(Z,0)\n","      Zaux = np.exp(Zaux)\n","      A = Zaux/np.sum(Zaux,0)\n","      activation_cache = Zaux\n","\n","  assert A.shape == (W.shape[0], A_prev.shape[1])\n","\n","  cache = (linear_cache, activation_cache)\n","\n","  return A, cache\n","\n","\n","def L_model_forward( X, parameters, hidden_layers_activation_fn=\"relu\"):\n","  \"\"\"\n","  Computes the output layer through looping over all units in topological\n","  order.\n","\n","  Arguments\n","  ---------\n","  X : 2d-array\n","      input matrix of shape input_size x training_examples.\n","  parameters : dict\n","      contains all the weight matrices and bias vectors for all layers.\n","  hidden_layers_activation_fn : str\n","      activation function to be used on hidden layers: \"tanh\", \"relu\".\n","\n","  Returns\n","  -------\n","  AL : 2d-array\n","      probability vector of shape 1 x training_examples.\n","  caches : list\n","      that contains L tuples where each layer has: A_prev, W, b, Z.\n","  \"\"\"\n","  A = X\n","  caches = []\n","  L = len(parameters) //2\n","\n","  for l in range(1, L):\n","    A_prev = A\n","    A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation_fn=hidden_layers_activation_fn)\n","    caches.append(cache)\n","\n","\n","  AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation_fn=\"softmax\")\n","  caches.append(cache)\n","\n","  # assert AL.shape == (1, X.shape[1])\n","\n","  return AL, caches\n","\n","# Compute cross-entropy cost\n","def compute_cost(AL, y, W, numLayers, lambda_reg):\n","  \"\"\"\n","  Computes the binary Cross-Entropy cost.\n","\n","  Arguments\n","  ---------\n","  AL : 2d-array\n","      probability vector of shape 1 x training_examples.\n","  y : 2d-array\n","      true \"label\" vector.\n","\n","  Returns\n","  -------\n","  cost : float\n","      binary cross-entropy cost.\n","  \"\"\"\n","  m = y.shape[1]\n","  # cost = - (1 / m)*np.sum(np.multiply(y, np.log(AL + 1e-30)) + np.multiply(1 - y, np.log(1 - AL)))\n","  cost = - (1 / m)*np.sum(np.multiply(y, np.log(AL + np.exp(-30))))\n","  regularization = 0\n","  for i in range(1,numLayers+1):\n","    regularization += np.sum(np.sum(np.square(W['W'+str(i)])))\n","\n","  cost += (1 / m)*(lambda_reg/2)*regularization\n","\n","  return cost\n","\n","# Define derivative of activation functions w.r.t z that will be used in back-propagation\n","def sigmoid_gradient(dA, Z):\n","  \"\"\"\n","  Computes the gradient of sigmoid output w.r.t input Z.\n","\n","  Arguments\n","  ---------\n","  dA : 2d-array\n","      post-activation gradient, of any shape.\n","  Z : 2d-array\n","      input used for the activation fn on this layer.\n","\n","  Returns\n","  -------\n","  dZ : 2d-array\n","      gradient of the cost with respect to Z.\n","  \"\"\"\n","  A, Z = sigmoid(Z)\n","  dZ = dA * A * (1 - A)\n","\n","  return dZ\n","\n","\n","def tanh_gradient(dA, Z):\n","  \"\"\"\n","  Computes the gradient of hyperbolic tangent output w.r.t input Z.\n","\n","  Arguments\n","  ---------\n","  dA : 2d-array\n","      post-activation gradient, of any shape.\n","  Z : 2d-array\n","      input used for the activation fn on this layer.\n","\n","  Returns\n","  -------\n","  dZ : 2d-array\n","      gradient of the cost with respect to Z.\n","  \"\"\"\n","  A, Z = tanh(Z)\n","  dZ = dA * (1 - np.square(A))\n","\n","  return dZ\n","\n","\n","def relu_gradient(dA, Z):\n","  \"\"\"\n","  Computes the gradient of ReLU output w.r.t input Z.\n","\n","  Arguments\n","  ---------\n","  dA : 2d-array\n","      post-activation gradient, of any shape.\n","  Z : 2d-array\n","      input used for the activation fn on this layer.\n","\n","  Returns\n","  -------\n","  dZ : 2d-array\n","      gradient of the cost with respect to Z.\n","  \"\"\"\n","  A, Z = relu(Z)\n","  dZ = np.multiply(dA, np.int64(A > 0))\n","\n","  return dZ\n","\n","\n","# define helper functions that will be used in L-model back-prop\n","def linear_backword(dZ, cache, reg):\n","  \"\"\"\n","  Computes the gradient of the output w.r.t weight, bias, and post-activation\n","  output of (l - 1) layers at layer l.\n","\n","  Arguments\n","  ---------\n","  dZ : 2d-array\n","      gradient of the cost w.r.t. the linear output (of current layer l).\n","  cache : tuple\n","      values of (A_prev, W, b) coming from the forward propagation in the current layer.\n","\n","  Returns\n","  -------\n","  dA_prev : 2d-array\n","      gradient of the cost w.r.t. the activation (of the previous layer l-1).\n","  dW : 2d-array\n","      gradient of the cost w.r.t. W (current layer l).\n","  db : 2d-array\n","      gradient of the cost w.r.t. b (current layer l).\n","  \"\"\"\n","  A_prev, W, b = cache\n","  m = A_prev.shape[1]\n","\n","  dW = (1 / m) * np.dot(dZ, A_prev.T) + (1/m)*reg*W\n","  db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n","  dA_prev = np.dot(W.T, dZ)\n","\n","  assert dA_prev.shape == A_prev.shape\n","  assert dW.shape == W.shape\n","  assert db.shape == b.shape\n","\n","  return dA_prev, dW, db\n","\n","\n","def linear_activation_backward(dA, cache, activation_fn, reg):\n","  \"\"\"\n","  Arguments\n","  ---------\n","  dA : 2d-array\n","      post-activation gradient for current layer l.\n","  cache : tuple\n","      values of (linear_cache, activation_cache).\n","  activation : str\n","      activation used in this layer: \"sigmoid\", \"tanh\", or \"relu\".\n","\n","  Returns\n","  -------\n","  dA_prev : 2d-array\n","      gradient of the cost w.r.t. the activation (of the previous layer l-1), same shape as A_prev.\n","  dW : 2d-array\n","      gradient of the cost w.r.t. W (current layer l), same shape as W.\n","  db : 2d-array\n","      gradient of the cost w.r.t. b (current layer l), same shape as b.\n","  \"\"\"\n","  linear_cache, activation_cache = cache\n","\n","  if activation_fn == \"sigmoid\":\n","      dZ = sigmoid_gradient(dA, activation_cache)\n","      dA_prev, dW, db = linear_backword(dZ, linear_cache, reg)\n","\n","  elif activation_fn == \"tanh\":\n","      dZ = tanh_gradient(dA, activation_cache)\n","      dA_prev, dW, db = linear_backword(dZ, linear_cache, reg)\n","\n","  elif activation_fn == \"relu\":\n","      dZ = relu_gradient(dA, activation_cache)\n","      dA_prev, dW, db = linear_backword(dZ, linear_cache, reg)\n","\n","  return dA_prev, dW, db\n","\n","\n","def L_model_backward(AL, y, caches, hidden_layers_activation_fn=\"relu\", reg=1e-2):\n","  \"\"\"\n","  Computes the gradient of output layer w.r.t weights, biases, etc. starting\n","  on the output layer in reverse topological order.\n","\n","  Arguments\n","  ---------\n","  AL : 2d-array\n","      probability vector, output of the forward propagation (L_model_forward()).\n","  y : 2d-array\n","      true \"label\" vector (containing 0 if non-cat, 1 if cat).\n","  caches : list\n","      list of caches for all layers.\n","  hidden_layers_activation_fn :\n","      activation function used on hidden layers: \"tanh\", \"relu\".\n","\n","  Returns\n","  -------\n","  grads : dict\n","      with the gradients.\n","  \"\"\"\n","  y = y.reshape(AL.shape)\n","\n","  L = len(caches)\n","  grads = {}\n","\n","  # dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n","\n","  # grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\n","  #     \"db\" + str(L)] = linear_activation_backward(\n","  #         dAL, caches[L - 1], \"sigmoid\", reg)\n","\n","  # for l in range(L - 1, 0, -1):\n","  #     current_cache = caches[l - 1]\n","  #     grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)], grads[\n","  #         \"db\" + str(l)] = linear_activation_backward(\n","  #             grads[\"dA\" + str(l)], current_cache,\n","  #             hidden_layers_activation_fn, reg)\n","\n","  m = y.shape[1]\n","  Wgradient = {}\n","  delta = -(1/m)*(y-AL)\n","\n","  for l in range(L - 1, -1, -1):\n","      current_cache = caches[l]\n","      linear_cache, activation_cache = current_cache\n","      A_prev, W, b = linear_cache\n","\n","      Wgradient[l] = np.hstack((b,W))\n","\n","      if l==(L-1):\n","        Wgradient[l] = np.dot(delta,np.vstack((np.ones((1,A_prev.shape[1])),A_prev)).T)\n","      else:\n","        Wgradient[l] = np.dot(delta[1:,:],np.vstack((np.ones((1,A_prev.shape[1])),A_prev)).T)\n","\n","      Wgradient[l][:,1:] += (1/m)*reg*W\n","\n","      grads[\"dW\"+str(l+1)] = Wgradient[l][:,1:]\n","      grads[\"db\"+str(l+1)] = np.reshape(Wgradient[l][:,0],(Wgradient[l][:,0].shape[0],1))\n","\n","      if l == 0:\n","        break\n","\n","      current_cache = caches[l-1]\n","      linear_cache, activation_cache = current_cache\n","      Z = activation_cache\n","\n","      if hidden_layers_activation_fn == \"tanh\":\n","        trans_func_der = 1 - np.square(2/(1+np.exp(-2*Z))-1)\n","\n","      if l < L-1:\n","        delta = np.multiply(delta[1:,:].T@np.hstack((b,W)),np.vstack((np.ones((1,A_prev.shape[1])),trans_func_der)).T)\n","      else:\n","        delta = np.multiply(delta.T@np.hstack((b,W)),np.vstack((np.ones((1,A_prev.shape[1])),trans_func_der)).T)\n","\n","      delta = delta.T\n","\n","\n","  return grads\n","\n","# define the function to update both weight matrices and bias vectors\n","def update_parameters(parameters, grads, learning_rate):\n","  \"\"\"\n","  Update the parameters' values using gradient descent rule.\n","\n","  Arguments\n","  ---------\n","  parameters : dict\n","      contains all the weight matrices and bias vectors for all layers.\n","  grads : dict\n","      stores all gradients (output of L_model_backward).\n","\n","  Returns\n","  -------\n","  parameters : dict\n","      updated parameters.\n","  \"\"\"\n","  L = len(parameters) // 2\n","\n","  for l in range(1, L + 1):\n","      parameters[\"W\" + str(l)] = parameters[\n","          \"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n","      parameters[\"b\" + str(l)] = parameters[\n","          \"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n","\n","  return parameters\n","\n","# Define the multi-layer model using all the helper functions we wrote before\n","\n","\n","def L_layer_model(X, y, layers_dims, learning_rate=0.01, num_iterations=3000, print_cost=False, hidden_layers_activation_fn=\"relu\", lambda_reg=1e-2):\n","  \"\"\"\n","  Implements multilayer neural network using gradient descent as the\n","  learning algorithm.\n","\n","  Arguments\n","  ---------\n","  X : 2d-array\n","      data, shape: number of examples x num_px * num_px * 3.\n","  y : 2d-array\n","      true \"label\" vector, shape: 1 x number of examples.\n","  layers_dims : list\n","      input size and size of each layer, length: number of layers + 1.\n","  learning_rate : float\n","      learning rate of the gradient descent update rule.\n","  num_iterations : int\n","      number of iterations of the optimization loop.\n","  print_cost : bool\n","      if True, it prints the cost every 100 steps.\n","  hidden_layers_activation_fn : str\n","      activation function to be used on hidden layers: \"tanh\", \"relu\".\n","\n","  Returns\n","  -------\n","  parameters : dict\n","      parameters learnt by the model. They can then be used to predict test examples.\n","  \"\"\"\n","  # np.random.seed(1)\n","\n","  # initialize parameters\n","  parameters = initialize_parameters(layers_dims)\n","\n","  # intialize cost list\n","  cost_list = []\n","\n","  ############MATLAB fmincg##############\n","  RHO   = 0.01                                                                     # a bunch of constants for line searches\n","  SIG   = 0.5                                                                      # RHO and SIG are the constants in the Wolfe-Powell conditions\n","  INT   = 0.1                                                                      # don't reevaluate within 0.1 of the limit of the current bracket\n","  EXT   = 3.0                                                                      # extrapolate maximum 3 times the current bracket\n","  MAX   = 20                                                                       # max 20 function evaluations per line search\n","  RATIO = 100                                                                      # maximum allowed slope ratio\n","\n","  L = len(parameters) // 2\n","\n","  red = 1\n","  S=['Iteration: ']\n","  df1_aux = np.empty(([]))\n","  for l in range(1, L + 1):\n","      df1_temp = np.hstack((parameters[\"b\" + str(l)],parameters[\"W\" + str(l)]))\n","      df1_aux = np.vstack((df1_aux,np.reshape(np.transpose(df1_temp),(np.shape(df1_temp)[0]*np.shape(df1_temp)[1],1))))\n","  W = df1_aux[1:]\n","\n","  ####################Ler um W especifico de um .csv###############\n","  # path = \"X.csv\"\n","\n","  # df = pd.read_csv(path, sep=r'\\s*,\\s*',header=None, encoding='ascii', engine='python')\n","  # df.columns = [\"X1\"]\n","  # W = df.values\n","  # SP = 0\n","  # for l in range(1, L + 1):\n","  #   EP = SP+np.shape(parameters[\"b\" + str(l)])[0]\n","  #   parameters[\"b\" + str(l)] = W[SP:EP]\n","  #   SP = EP\n","  #   EP = SP+np.shape(parameters[\"W\" + str(l)])[0]*np.shape(parameters[\"W\" + str(l)])[1]\n","  #   parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[0],np.shape(parameters[\"W\" + str(l)])[1])))\n","  #   SP = EP\n","  #############################################################\n","\n","  # i = 0                                                                            # zero the run length counter\n","  ls_failed = 0                                                                    # no previous line search has failed\n","  fW = np.empty(([]))\n","\n","  # iterate over L-layers to get the final output and the cache\n","  AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n","\n","  # compute cost to plot it\n","  cost = compute_cost(AL, y, parameters, len(parameters)//2, lambda_reg)\n","\n","  # iterate over L-layers backward to get gradients\n","  grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn, lambda_reg)\n","\n","  f1 = cost\n","  df1_aux = np.empty(([]))\n","  for l in range(1, L + 1):\n","      df1_temp = np.hstack((grads[\"db\" + str(l)],grads[\"dW\" + str(l)]))\n","      df1_aux = np.vstack((df1_aux,np.reshape(np.transpose(df1_temp),(np.shape(df1_temp)[0]*np.shape(df1_temp)[1],1))))\n","  df1 = df1_aux[1:]\n","  # print(f1)\n","  # print(df1)\n","  # input(\"Press Enter to continue...\")\n","  # i = i + (length<0);                                                            # count epochs?!\n","  s = -df1                                                                         # search direction is steepest\n","  d1 = np.dot(-np.transpose(s),s)                                                                # this is the slope\n","  z1 = red/(1-d1)                                                                  # initial step is red/(|s|+1)\n","\n","  # iterate over num_iterations\n","  for i in range(num_iterations):\n","      # # iterate over L-layers to get the final output and the cache\n","      # AL, caches = L_model_forward(\n","      #     X, parameters, hidden_layers_activation_fn)\n","\n","      # # compute cost to plot it\n","      # cost = compute_cost(AL, y, parameters, len(parameters)//2, lambda_reg)\n","      # # iterate over L-layers backward to get gradients\n","      # grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn, lambda_reg)\n","\n","      # # update parameters\n","      # parameters = update_parameters(parameters, grads, learning_rate)\n","\n","      ############MATLAB fmincg#############\n","      W0 = W; f0 = f1; df0 = df1                                                 # make a copy of current values\n","      W = W + z1*s;                                                              # begin line search\n","\n","      SP = 0\n","      for l in range(1, L + 1):\n","        EP = SP+np.shape(parameters[\"b\" + str(l)])[0]\n","        parameters[\"b\" + str(l)] = W[SP:EP]\n","        SP = EP\n","        EP = SP+np.shape(parameters[\"W\" + str(l)])[0]*np.shape(parameters[\"W\" + str(l)])[1]\n","        # parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[0],np.shape(parameters[\"W\" + str(l)])[1])))\n","        parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[1],np.shape(parameters[\"W\" + str(l)])[0])))\n","        SP = EP\n","\n","      # iterate over L-layers to get the final output and the cache\n","      #aqui\n","      AL, caches = L_model_forward(\n","          X, parameters, hidden_layers_activation_fn)\n","\n","      # compute cost to plot it\n","      cost = compute_cost(AL, y, parameters, len(parameters)//2, lambda_reg)\n","\n","      # iterate over L-layers backward to get gradients\n","      grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn, lambda_reg)\n","\n","      f2 = cost\n","      df1_aux = np.empty(([]))\n","      for l in range(1, L + 1):\n","          df1_temp = np.hstack((grads[\"db\" + str(l)],grads[\"dW\" + str(l)]))\n","          df1_aux = np.vstack((df1_aux,np.reshape(np.transpose(df1_temp),(np.shape(df1_temp)[0]*np.shape(df1_temp)[1],1))))\n","      df2 = df1_aux[1:]\n","\n","      # i = i + (num_iterations<0);                                               # count epochs?!\n","      d2 = np.dot(np.transpose(df2),s);\n","      f3 = f1; d3 = d1; z3 = -z1;                                               # initialize point 3 equal to point 1\n","      if num_iterations>0:\n","        M = MAX\n","      else:\n","        M = min(MAX, -num_iterations-i)\n","      success = 0; limit = -1                                                   # initialize quanteties\n","\n","      while True:\n","        while ((f2 > f1+z1*RHO*d1) or (d2 > -SIG*d1)) and (M > 0):\n","          limit = z1                                                            # tighten the bracket\n","          if f2 > f1:\n","            z2 = z3 - (0.5*d3*z3*z3)/(d3*z3+f2-f3)                              # quadratic fit\n","          else:\n","            A = 6*(f2-f3)/z3+3*(d2+d3)                                          # cubic fit\n","            B = 3*(f3-f2)-z3*(d3+2*d2)\n","            z2 = (np.sqrt(B*B-A*d2*z3*z3)-B)/A                                  # numerical error possible - ok!\n","\n","          if np.isnan(z2) or np.isinf(z2):\n","            z2 = z3/2                                                           # if we had a numerical problem then bisect\n","\n","          z2 = max(min(z2, INT*z3),(1-INT)*z3)                                  # don't accept too close to limits\n","          z1 = z1 + z2                                                          # update the step\n","          W = W + z2*s\n","\n","          SP = 0\n","          for l in range(1, L + 1):\n","            EP = SP+np.shape(parameters[\"b\" + str(l)])[0]\n","            parameters[\"b\" + str(l)] = W[SP:EP]\n","            SP = EP\n","            EP = SP+np.shape(parameters[\"W\" + str(l)])[0]*np.shape(parameters[\"W\" + str(l)])[1]\n","            # parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[0],np.shape(parameters[\"W\" + str(l)])[1])))\n","            parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[1],np.shape(parameters[\"W\" + str(l)])[0])))\n","            SP = EP\n","\n","          # iterate over L-layers to get the final output and the cache\n","          AL, caches = L_model_forward(X, parameters, hidden_layers_activation_fn)\n","\n","          # compute cost to plot it\n","          cost = compute_cost(AL, y, parameters, len(parameters)//2, lambda_reg)\n","\n","          # iterate over L-layers backward to get gradients\n","          grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn, lambda_reg)\n","\n","          f2 = cost\n","          df1_aux = np.empty(([]))\n","          for l in range(1, L + 1):\n","              df1_temp = np.hstack((grads[\"db\" + str(l)],grads[\"dW\" + str(l)]))\n","              df1_aux = np.vstack((df1_aux,np.reshape(np.transpose(df1_temp),(np.shape(df1_temp)[0]*np.shape(df1_temp)[1],1))))\n","          df2 = df1_aux[1:]\n","\n","          M = M - 1\n","          # i = i + (length<0);                           % count epochs?!\n","          d2 = np.dot(np.transpose(df2),s)\n","          z3 = z3-z2                                                            # z3 is now relative to the location of z2\n","\n","        if f2 > f1+z1*RHO*d1 or d2 > -SIG*d1:\n","          break                                                                 # this is a failure\n","        elif d2 > SIG*d1:\n","          success = 1; break                                                    # success\n","        elif M == 0:\n","          break                                                                 # failure\n","\n","        A = 6*(f2-f3)/z3+3*(d2+d3)                                              # make cubic extrapolation\n","        B = 3*(f3-f2)-z3*(d3+2*d2)\n","        z2 = -d2*z3*z3/(B+np.sqrt(B*B-A*d2*z3*z3))                              # num. error possible - ok!\n","        if ~np.isreal(z2) or np.isnan(z2) or np.isinf(z2) or z2 < 0:            # num prob or wrong sign?\n","          if limit < -0.5:                                                      # if we have no upper limit\n","            z2 = z1 * (EXT-1)                                                   # the extrapolate the maximum amount\n","          else:\n","            z2 = (limit-z1)/2                                                   # otherwise bisect\n","\n","        elif (limit > -0.5) and (z2+z1 > limit):                                # extraplation beyond max?\n","          z2 = (limit-z1)/2                                                     # bisect\n","        elif (limit < -0.5) and (z2+z1 > z1*EXT):                               # extrapolation beyond limit\n","          z2 = z1*(EXT-1.0)                                                     # set to extrapolation limit\n","        elif z2 < -z3*INT:\n","          z2 = -z3*INT\n","        elif (limit > -0.5) and (z2 < (limit-z1)*(1.0-INT)):                    # too close to limit?\n","          z2 = (limit-z1)*(1.0-INT)\n","\n","        f3 = f2; d3 = d2; z3 = -z2                                              # set point 3 equal to point 2\n","        z1 = z1 + z2\n","        W = W + z2*s                                                            # update current estimates\n","\n","        SP = 0\n","        for l in range(1, L + 1):\n","          EP = SP+np.shape(parameters[\"b\" + str(l)])[0]\n","          parameters[\"b\" + str(l)] = W[SP:EP]\n","          SP = EP\n","          EP = SP+np.shape(parameters[\"W\" + str(l)])[0]*np.shape(parameters[\"W\" + str(l)])[1]\n","          # parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[0],np.shape(parameters[\"W\" + str(l)])[1])))\n","          parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[1],np.shape(parameters[\"W\" + str(l)])[0])))\n","          SP = EP\n","\n","        # iterate over L-layers to get the final output and the cache\n","        AL, caches = L_model_forward(\n","            X, parameters, hidden_layers_activation_fn)\n","\n","        # compute cost to plot it\n","        cost = compute_cost(AL, y, parameters, len(parameters)//2, lambda_reg)\n","\n","        # iterate over L-layers backward to get gradients\n","        grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn, lambda_reg)\n","\n","        f2 = cost\n","        df1_aux = np.empty(([]))\n","        for l in range(1, L + 1):\n","            df1_temp = np.hstack((grads[\"db\" + str(l)],grads[\"dW\" + str(l)]))\n","            df1_aux = np.vstack((df1_aux,np.reshape(np.transpose(df1_temp),(np.shape(df1_temp)[0]*np.shape(df1_temp)[1],1))))\n","        df2 = df1_aux[1:]\n","        M = M - 1\n","        # i = i + (length<0);                             % count epochs?!\n","        d2 = np.dot(np.transpose(df2),s)\n","        # end of line search\n","\n","      if success:                                                               # if line search succeeded\n","        f1 = f2; fW = np.vstack((fW,f1))\n","        # print('%s %4i | Cost: %4.6e\\r', S, i, f1)\n","        s = (np.dot(np.transpose(df2),df2)-np.dot(np.transpose(df1),df2))/(np.dot(np.transpose(df1),df1))*s - df2         # Polack-Ribiere direction\n","        tmp = df1; df1 = df2; df2 = tmp                                         # swap derivatives\n","        d2 = np.dot(np.transpose(df1),s)\n","        if d2 > 0:                                                              # new slope must be negative\n","          s = -df1                                                              # otherwise use steepest direction\n","          d2 = np.dot(-np.transpose(s),s)\n","\n","        z1 = z1 * min(RATIO, d1/(d2-np.finfo(float).tiny))                      # slope ratio but max RATIO\n","        d1 = d2\n","        ls_failed = 0                                                           # this line search did not fail\n","      else:\n","        W = W0; f1 = f0; df1 = df0                                              # restore point from before failed line search\n","        if ls_failed or i > abs(num_iterations):                                # line search failed twice in a row\n","          break                                                                 # or we ran out of time, so we give up\n","\n","        tmp = df1; df1 = df2; df2 = tmp                                         # swap derivatives\n","        s = -df1                                                                # try steepest\n","        d1 = np.dot(-np.transpose(s),s)\n","        z1 = 1/(1-d1)\n","        ls_failed = 1                                                           # this line search failed\n","\n","      # append each 100th cost to the cost list\n","      if (i + 1) % 100 == 0 and print_cost:\n","          print(f\"The cost after {i + 1} iterations is: {cost:.4f}\")\n","\n","      if i % 100 == 0:\n","          cost_list.append(cost)\n","\n","  # plot the cost curve\n","  # plt.figure(figsize=(10, 6))\n","  # plt.plot(cost_list)\n","  # plt.xlabel(\"Iterations (per hundreds)\")\n","  # plt.ylabel(\"Loss\")\n","  # plt.title(f\"Loss curve for the learning rate = {learning_rate}\")\n","\n","  SP = 0\n","  for l in range(1, L + 1):\n","    EP = SP+np.shape(parameters[\"b\" + str(l)])[0]\n","    parameters[\"b\" + str(l)] = W[SP:EP]\n","    SP = EP\n","    EP = SP+np.shape(parameters[\"W\" + str(l)])[0]*np.shape(parameters[\"W\" + str(l)])[1]\n","    # parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[0],np.shape(parameters[\"W\" + str(l)])[1])))\n","    parameters[\"W\" + str(l)] = np.transpose(np.reshape(W[SP:EP],(np.shape(parameters[\"W\" + str(l)])[1],np.shape(parameters[\"W\" + str(l)])[0])))\n","    SP = EP\n","\n","  return parameters\n","\n","# def accuracy(X, parameters, y, activation_fn=\"relu\"):\n","#   \"\"\"\n","#   Computes the average accuracy rate.\n","\n","#   Arguments\n","#   ---------\n","#   X : 2d-array\n","#       data, shape: number of examples x num_px * num_px * 3.\n","#   parameters : dict\n","#       learnt parameters.\n","#   y : 2d-array\n","#       true \"label\" vector, shape: 1 x number of examples.\n","#   activation_fn : str\n","#       activation function to be used on hidden layers: \"tanh\", \"relu\".\n","\n","#   Returns\n","#   -------\n","#   accuracy : float\n","#       accuracy rate after applying parameters on the input data\n","#   \"\"\"\n","#   probs, caches = L_model_forward(X, parameters, activation_fn)\n","#   labels = (probs >= 0.5) * 1\n","#   accuracy = np.mean(labels == y) * 100\n","\n","#   return f\"The accuracy rate is: {accuracy:.2f}%.\"\n"]},{"cell_type":"code","source":["# data reader\n","\n","import numpy as np\n","import pandas as pd\n","\n","def data_reader(users, gestures, version):\n","\n","  subjects = []\n","\n","  # gestures_code = pd.DataFrame(gestures,columns=['Gestures'])\n","\n","  if version == 'training':\n","    flag_name = ['TrialOne','TrialTwo','TrialThree','TrialFour','TrialFive']\n","\n","  elif version == 'testing':\n","    flag_name = ['TrialOne','TrialTwo','TrialThree','TrialFour','TrialFive','TrialSix','TrialSeven','TrialEight',\n","                'TrialNine','TrialTen','TrialEleven','TrialTwelve','TrialThirteen','TrialFourteen','TrialFifteen',\n","                'TrialSixteen','TrialSeventeen','TrialEighteen','TrialNineteen','TrialTwenty','TrialTwentyOne',\n","                'TrialTwentyTwo','TrialTwentyThree','TrialTwentyFour','TrialTwentyFive','TrialTwentySix','TrialTwentySeven',\n","                'TrialTwentyEight','TrialTwentyNine','TrialThirty']\n","\n","    if 'relax' in gestures:\n","      gestures = gestures[1:]\n","\n","  else:\n","    raise ValueError(\"Parameter 'version' must be either 'training' or 'testing'.\")\n","\n","  for user in users:\n","    subject_data = []\n","\n","    for gesture in gestures:\n","      if version == 'training':\n","        path = user + '_' + gesture + '.csv'\n","      else:\n","        path = user + '_' + gesture + ' (1).csv'\n","\n","      df = pd.read_csv(path,\n","                       sep=r'\\s*,\\s*',\n","                       header=0,\n","                       encoding='ascii',\n","                       engine='python')\n","\n","      flag = df.loc[df['ChannelOne'].isin(flag_name)]\n","\n","      flag_index = np.array(flag.index.values)\n","      flag_index = np.append(flag_index,len(df))\n","\n","      trial_semg = [trial(df,flag_index,i) for i in range(len(flag_index)-1)]\n","\n","      subject_data.append(\n","          dict(\n","              gesture = gesture,\n","              trials = trial_semg\n","          )\n","      )\n","\n","    subjects.append(\n","        dict(\n","            name = user,\n","            data = subject_data\n","        )\n","    )\n","\n","  return subjects"],"metadata":{"id":"cUCafDsMs_G5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set split\n","\n","import numpy as np\n","import copy\n","import random\n","\n","def set_split(data_train, data_test, validation, val_split):\n","\n","  N_users = len(data_train) # Acquire the quantity of users in the dataset\n","\n","  data_splitted = []\n","\n","  if validation == 'mhold':\n","\n","    for subject_i in range(N_users):\n","\n","      train_data_splitted = []\n","      val_data_splitted   = []\n","      test_data_splitted  = []\n","\n","      for gesture_data in data_train[subject_i]['data']:\n","        train_data = [trial_data.to_numpy().astype(float) for trial_data in gesture_data['trials']]\n","\n","        train_data_splitted.append(\n","            dict(\n","                gesture = gesture_data['gesture'],\n","                data = train_data\n","            )\n","        )\n","\n","      for gesture_data in data_test[subject_i]['data']:\n","        val_len = val_split*len(gesture_data['trials'])\n","        val_len = int(np.floor(val_len))\n","\n","        val_data  = [trial_data.to_numpy().astype(float) for trial_data in gesture_data['trials'][:val_len]]\n","\n","        val_data_splitted.append(\n","            dict(\n","                gesture = gesture_data['gesture'],\n","                data = val_data\n","            )\n","        )\n","        test_data = [trial_data.to_numpy().astype(float) for trial_data in gesture_data['trials'][val_len:]]\n","\n","        test_data_splitted.append(\n","            dict(\n","                gesture = gesture_data['gesture'],\n","                data = test_data\n","            )\n","        )\n","\n","      data_splitted.append(\n","          dict(\n","              name = data_train[subject_i]['name'],\n","              train = train_data_splitted,\n","              val = val_data_splitted,\n","              test = test_data_splitted\n","          )\n","      )\n","\n","    return data_splitted"],"metadata":{"id":"0aAqYlJIDqwt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preproc new\n","\n","# from scipy import signal\n","import numpy as np\n","# import pandas as pd\n","# import copy\n","# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","# from sklearn.svm import SVC\n","\n","# import pickle\n","\n","def preproc_new(data, train_params, tau):\n","\n","  a             = train_params['a']\n","  b             = train_params['b']\n","  fs            = train_params['fs']\n","  window        = train_params['window']\n","  numFreqOfSpec = train_params['numFreqOfSpec']\n","  nOver_win     = train_params['nOver_win']\n","  hamm_win      = train_params['hamm_win']\n","  min_seg       = train_params['min_seg']\n","\n","  best_centers = []\n","  single_set   = []\n","  Y_data       = []\n","\n","  for idx_gesture, gesture_data in enumerate(data):\n","    N_trial = len(gesture_data['data'])\n","    Y_data.append(idx_gesture+1)\n","\n","    # Filtered signal\n","    filt_data = filt_emg(gesture_data['data'],b,a)\n","\n","    # Detect muscle activity\n","    [seg_data,_,_,idx_s,idx_e] = muscle_activity(filt_data,\n","                                                 fs,\n","                                                 window,\n","                                                 numFreqOfSpec,\n","                                                 nOver_win,\n","                                                 hamm_win,\n","                                                 tau,\n","                                                 min_seg)\n","\n","    # # Best center for each gesture\n","    # best_centers.append(best_center_class(seg_data))\n","\n","    # Best center for each gesture using dtaidtw\n","    best_centers.append(best_center_dtai(seg_data))\n","\n","    for d in seg_data:\n","      single_set.append(d)\n","\n","  # Single set\n","  single_set = np.array(single_set, dtype=object)\n","\n","  # Data label\n","  Y_data = np.repeat(np.array(Y_data), N_trial)\n","\n","  return single_set, best_centers, Y_data"],"metadata":{"id":"72MaUN5GYi7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feature extractor\n","\n","import numpy as np\n","\n","def feature_extractor(data, best_centers):\n","\n","  feat_data = []\n","\n","  # Create the DTW between the input and the centers of each class\n","  for best_center in best_centers:\n","    dtw_data = dtw_extract(data, best_center)\n","    feat_data.append(dtw_data)\n","\n","  feat_data = np.stack(feat_data, axis=1) # Convert the list of arrays to a single array\n","\n","  # Normalizes the feature vector\n","  feat_data = feat_normalization(feat_data)\n","\n","  return feat_data"],"metadata":{"id":"8bZI8WcECNMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training classifier\n","\n","import numpy as np\n","\n","def training_classifier(dataX_train, label, layers_dims, reg, mode):\n","\n","  label_unique   = np.unique(label)  # How many unique labels are in the training dataset\n","  N_label_unique = len(label_unique) # How many classes are in total in the training dataset\n","\n","  X_train = np.transpose(dataX_train)\n","  Y_train = np.eye(N_label_unique)[label-1]\n","  Y_train = np.transpose(Y_train)\n","\n","  if mode == 'mlp_simple':\n","    parameters_tanh = L_layer_model(\n","                                    X_train,\n","                                    Y_train,\n","                                    layers_dims,\n","                                    learning_rate=0.01,\n","                                    num_iterations=50000,\n","                                    hidden_layers_activation_fn=\"tanh\",\n","                                    lambda_reg=reg\n","                                    )\n","\n","  else:\n","    raise ValueError(\"Parameter model must be 'mlp_simple'.\")\n","\n","  return parameters_tanh"],"metadata":{"id":"gVzbOeQVvBLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# new predictor\n","\n","import numpy as np\n","# import pandas as pd\n","# import pickle\n","\n","def predictor_new(data, best_centers, clf, params, tau, post='poll'):\n","\n","  # Parameters definition\n","  a             = params['a']\n","  b             = params['b']\n","  fs            = params['fs']\n","  window        = params['window']\n","  numFreqOfSpec = params['numFreqOfSpec']\n","  nOver_win     = params['nOver_win']\n","  hamm_win      = params['hamm_win']\n","  min_seg       = params['min_seg']\n","  stride        = params['stride']\n","  win_len       = params['win_len']\n","\n","  X_cache = []\n","\n","  post_preds = []\n","\n","  for gesture_data in data:\n","    X_cache_gesture = []\n","\n","    post_preds_gestures = []\n","\n","    for trial_data in gesture_data['data']:\n","      samples = len(trial_data) # Total samples in the trial\n","\n","      start_point = 0\n","      win_count   = 0\n","\n","      trial_preds = []\n","\n","      X_cache_trial = []\n","\n","      while True:\n","        start_point = win_count*stride\n","        end_point   = start_point + win_len\n","\n","        # Exit if the time window pass the last trial's sample\n","        if end_point > samples:\n","          break\n","\n","        # Selecting only a time window of size win_len from the trial\n","        windowed_data = trial_data[start_point:end_point]\n","\n","        # Filtering the signal\n","        filt_data = filt_emg([windowed_data],b,a)\n","\n","        # Detecting muscle activity\n","        [_,_,_,idx_s,idx_e] = muscle_activity(filt_data,\n","                                              fs,\n","                                              window,\n","                                              numFreqOfSpec,\n","                                              nOver_win,\n","                                              hamm_win,\n","                                              tau,\n","                                              min_seg)\n","\n","        # Check if any muscle activity was detected\n","        # Otherwise, the prediction is no-gesture\n","        if idx_s[0] !=0 and idx_e[0] != len(windowed_data):\n","          windowed_data = windowed_data[idx_s[0]:idx_e[0]]\n","          filt_windowed_data = filt_emg([windowed_data],b,a)\n","\n","          # Extracting and normalizing the features from the windowed data\n","          X = feature_extractor(filt_windowed_data, best_centers)\n","          X = np.transpose(X)\n","\n","          # Saving a cache to decrease processing time when using ensemble\n","          X_cache_trial.append(X)\n","\n","          # Fowarding the data into the net\n","          preds, caches = L_model_forward(X,\n","                                          clf,\n","                                          hidden_layers_activation_fn=\"tanh\"\n","                                          )\n","\n","          # We only consider predictions with confidence higher than 50%\n","          # Otherwise, the prediction is classified as no-gesture\n","          if max(preds) > 0.5:\n","            trial_preds.append(np.argmax(preds)+1)\n","          else:\n","            trial_preds.append(1)\n","\n","        else:\n","          # If there is no muscle activity, the cache receives 0s\n","          X_cache_trial.append(np.zeros((6,1)))\n","          trial_preds.append(1)\n","\n","        win_count += 1\n","\n","      # Choose among the post processing methods\n","      if post == 'poll':\n","        post_pred = poll_postprocessing(trial_preds)\n","        post_preds_gestures.append(post_pred)\n","\n","      elif post == 'base':\n","        post_pred = base_postprocessing(trial_preds)\n","        post_preds_gestures.append(post_pred)\n","\n","      else:\n","        raise ValueError(\"The parameter post must be either 'poll' or 'base'.\")\n","\n","      X_cache_gesture.append(\n","          dict(\n","              trial = X_cache_trial\n","          )\n","      )\n","\n","    post_preds.append(\n","        dict(\n","            preds = post_preds_gestures\n","        )\n","    )\n","\n","    X_cache.append(\n","        dict(\n","            data = X_cache_gesture\n","        )\n","    )\n","\n","  return post_preds, X_cache"],"metadata":{"id":"eJ0vhr2cAIxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# new predictor for ensemble processing\n","\n","import numpy as np\n","\n","def predictor_ens_new(data, clf, post='poll'):\n","\n","  post_preds = []\n","\n","  for gesture_data in data:\n","\n","    post_preds_gestures = []\n","\n","    for trial_data in gesture_data['data']:\n","\n","      trial_preds = []\n","\n","      for window_data in trial_data['trial']:\n","\n","        if window_data.all() != 0:\n","          X = window_data\n","\n","          preds, caches = L_model_forward(X,\n","                                          clf,\n","                                          hidden_layers_activation_fn=\"tanh\"\n","                                          )\n","\n","          # We only consider predictions with confidence higher than 50%\n","          # Otherwise, the prediction is classified as no-gesture\n","          if max(preds) > 0.5:\n","            trial_preds.append(np.argmax(preds)+1)\n","          else:\n","            trial_preds.append(1)\n","\n","        else:\n","          # If the window data in cache are all 0s, then predictions equal no-gesture\n","          trial_preds.append(1)\n","\n","      # Choose among the post processing methods\n","      if post == 'poll':\n","        post_pred = poll_postprocessing(trial_preds)\n","        post_preds_gestures.append(post_pred)\n","\n","      elif post == 'base':\n","        post_pred = base_postprocessing(trial_preds)\n","        post_preds_gestures.append(post_pred)\n","\n","      else:\n","        raise ValueError(\"The parameter post must be either 'poll' or 'base'.\")\n","\n","    post_preds.append(\n","          dict(\n","              preds = post_preds_gestures\n","          )\n","      )\n","\n","  return post_preds"],"metadata":{"id":"52TVPKCam_lp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# poll post processing\n","\n","def poll_postprocessing(preds):\n","\n","  N_labels = len(preds)\n","  labels   = preds\n","\n","  # Filter consecutive labels\n","  for label_i in range(1,N_labels):\n","    cond = preds[label_i] == preds[label_i - 1]\n","    labels[label_i]  = 1*cond + preds[label_i]*(1 - cond)\n","\n","  if all(label == labels[0] for label in labels) and labels[0] == 1:\n","    pred = 1\n","  else:\n","    labels = [label for label in labels if label != 1] # Removing the no-gesture label\n","    pred   = max(set(labels), key=labels.count)\n","\n","  return pred"],"metadata":{"id":"LSsXlazibw6i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# baseline post processing\n","\n","def base_postprocessing(preds):\n","\n","  N_labels = len(preds)\n","  labels   = preds\n","\n","  # Filter consecutive labels\n","  for label_i in range(1,N_labels):\n","    cond = preds[label_i] == preds[label_i-1]\n","    labels[label_i]  = 1*cond + preds[label_i]*(1 - cond)\n","\n","  # Removing the no-gesture label\n","  unique_labels = list(set(labels))\n","  unique_labels = [label for label in unique_labels if label != 1]\n","\n","  if len(unique_labels) == 0:\n","    pred = 1\n","  elif len(unique_labels) > 1:\n","    pred = unique_labels[0]\n","  else:\n","    pred = unique_labels\n","\n","  return pred[0]"],"metadata":{"id":"ETT_jMjCjewc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# label extractor\n","\n","def label_extractor(data, gestures):\n","\n","  labels = []\n","\n","  for gesture_data in data:\n","    label = gestures.index(gesture_data['gesture'])\n","\n","    labels.append(\n","        dict(\n","            labels = [label+1]*len(gesture_data['data'])\n","        )\n","    )\n","\n","  return labels"],"metadata":{"id":"j6XT6son-DqV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# new accuracy\n","\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","\n","def accuracy_new(preds, labels):\n","\n","  labels = [label['labels'] for label in labels]  # Converting the label dict to a list\n","  labels = sum(labels, [])                        # Flattening the labels list\n","\n","  preds = [pred['preds'] for pred in preds] # Converting the pred dict to a list\n","  preds = sum(preds, [])                    # Flattening the preds list\n","\n","  # Creating the confusion matrix\n","  cm = confusion_matrix(labels,preds)\n","\n","  # Calculating the accuracy\n","  acc = np.trace(cm)/np.sum(cm)\n","\n","  return acc"],"metadata":{"id":"i27j82sVuO9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find the largest plateau\n","\n","import numpy as np\n","\n","def find_largest_plateau(arr):\n","\n","  max_val = np.max(arr)\n","  max_indices = np.where(arr == max_val)[0]\n","\n","  # Initialize variables to track the largest plateau\n","  start = max_indices[0]\n","  end = max_indices[0]\n","  max_length = 1\n","\n","  current_start = max_indices[0]\n","  current_length = 1\n","\n","  # Iterate over the indices of the max values to find continuous segments\n","  for i in range(1, len(max_indices)):\n","\n","    if max_indices[i] == max_indices[i - 1] + 1:\n","      current_length += 1\n","      current_end = max_indices[i]\n","\n","    else:\n","      current_start = max_indices[i]\n","      current_length = 1\n","      current_end = max_indices[i]\n","\n","    # Update the max length and positions only if the current length is greater\n","    # or if it's equal to the max length but appears later\n","    if current_length >= max_length:\n","      max_length = current_length\n","      start = current_start\n","      end = current_end\n","\n","  return arr[start:end+1], start, end"],"metadata":{"id":"Fjvi6aDAbVm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving variables\n","\n","import pickle\n","\n","def save_var(var, name):\n","\n","  with open(name+'.pkl', \"wb\") as f:\n","            pickle.dump(var, f)"],"metadata":{"id":"uxQsZC5DY2CX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# main\n","\n","# IMPORTING THE LIBRARIES\n","import numpy as np\n","from scipy import signal\n","from operator import itemgetter\n","import os\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"-lE-S9fvC5hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(1)\n","random.seed(100)\n","\n","# Defining some constants\n","fs            = 200                                                              #Sample frequency\n","min_seg       = 100                                                              #Minimun segmentation length\n","numFreqOfSpec = 25                                                               #Number of frequencies for the STFT\n","hamm_win      = 25                                                               #Size of the Hamming window\n","nOver_win     = 10                                                               #Number of samples to overlap in the Hamming window\n","tau_u         = np.linspace(10,20,11)                                            #Constant to detect the muscle activity\n","filt_order    = 5                                                                #Filter of 5th order\n","filt_freq     = 0.1                                                              #Filter with cutoff frequency of 10Hz\n","stride        = 10\n","win_len       = 700\n","layers_dims   = [6, 8, 6]\n","reg           = 1e-2\n","N_ens         = 100\n","# tau_val       = np.array(([11., 13., 15., 15., 16., 18., 17., 10., 13., 19.]))\n","tau_val       = np.array(([11, 13., 15., 15., 16., 18., 13., 10., 13., 19.]))    # tau calculating by using fastdtw\n","\n","# tau_val       = None\n","N_mhold       = 10\n","mode          = 'mlp_simple'\n","post          = 'poll'\n","\n","validation = 'mhold'\n","val_split  = 0.2\n","\n","b, a = signal.butter(filt_order, filt_freq, 'low')                               #Butterworth lowpass filter of \"filt_order\" order and cutoff frequency of \"filt_freq\" (normalized)\n","\n","window = signal.windows.hamming(hamm_win)                                        #Hamming window\n","\n","params = {'a':a,\n","          'b':b,\n","          'fs':fs,\n","          'min_seg':min_seg,\n","          'numFreqOfSpec':numFreqOfSpec,\n","          'hamm_win':hamm_win,\n","          'nOver_win':nOver_win,\n","          'window':window,\n","          'stride':stride,\n","          'win_len':win_len}\n","\n","users = ['alejandroFlores',\n","        'alexandraApellido',\n","        'andresGuerra',\n","        'andresJaramillo',\n","        'cristhianMotoche',\n","        'dianitaCherrez',\n","        'homeroApellido',\n","        'jonathanZea',\n","        'juanYuquilema',\n","        'santiagoJaramillo']\n","\n","# users = ['santiagoJaramillo']\n","\n","gestures = ['relax',\n","            'fist',\n","            'wave_in',\n","            'wave_out',\n","            'fingers_spread',\n","            'double_tap']"],"metadata":{"id":"glJXEsKlDDau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(1)\n","random.seed(100)\n","\n","N_ens   = 100\n","# N_mhold = 5\n","# tau_u         = np.linspace(5,25,201)\n","# tau_u   = [17]\n","\n","# mode = 'mlp_simple'\n","# post = 'poll'\n","\n","# validation = 'mhold'\n","# val_split  = 0.2\n","\n","X_val_ens  = []\n","X_test_ens = []\n","\n","tau_best = []\n","\n","acc_exp = []\n","\n","version        = 'training'\n","raw_data_train = data_reader(users, gestures, version)\n","version        = 'testing'\n","raw_data_test  = data_reader(users, gestures, version)\n","\n","# Splitting the data into training, validation and testing sets\n","data_splitted = set_split(raw_data_train, raw_data_test, validation, val_split)\n","\n","for ens in range(N_ens):\n","  acc_users = []\n","\n","  # Iterating over all users\n","  for idx_user, user in enumerate(users):\n","\n","    # It is only necessary to compute the best tau in the firt ensemble iteration\n","    if ens > 0:\n","      tau_u = [tau_best[idx_user]]\n","\n","    # Experimenting with specific tau_u values\n","    if tau_val is not None:\n","      tau_u = [tau_val[idx_user]]\n","\n","    # Choose validation method\n","    if validation == 'mhold':\n","\n","      net_fitted_tau  = []\n","      acc_tau         = []\n","      X_tau           = []\n","\n","      # Validation process\n","      # Search for the best tau\n","      for tau in tau_u:\n","\n","        net_fitted_val = []\n","        acc_val        = []\n","\n","        # Computing the best centers for each class\n","        data_train = data_splitted[idx_user]['train']\n","        single_set, best_centers, dataY_train = preproc_new(data_train, params, tau)\n","\n","        # Extracting and normalizing the features from the data\n","        feat_dataX = feature_extractor(single_set, best_centers)\n","\n","        # Choose the best net among those trained by the multi-holdout method\n","        for mhold_n in range(N_mhold):\n","\n","          # Fit the network\n","          net_fitted_mhold = training_classifier(feat_dataX, dataY_train, layers_dims, reg, mode)\n","          # Store the network fitted in this mhold iteration\n","          net_fitted_val.append(net_fitted_mhold)\n","\n","          # Predicting in the validation set\n","          # In the first iteration of mhold, compute the X\n","          # In subsequent iterations of mhold, use X_cache to speed the process\n","          if mhold_n == 0 and ens == 0:\n","            data_val = data_splitted[idx_user]['val']\n","            predictions_new, X_val_cache = predictor_new(data_val,\n","                                                        best_centers,\n","                                                        net_fitted_mhold,\n","                                                        params,\n","                                                        tau,\n","                                                        post=post)\n","\n","            X_tau.append(X_val_cache)\n","\n","          else:\n","            if ens > 0:\n","              X_val_cache = X_val_ens[idx_user]\n","\n","            predictions_new = predictor_ens_new(X_val_cache,\n","                                                net_fitted_mhold,\n","                                                post=post)\n","\n","          # Extracting the labels from the validation data\n","          labels = label_extractor(data_val, gestures)\n","\n","          # Computing the validation accuracy\n","          acc_mhold = accuracy_new(predictions_new, labels)\n","          # Store the accuracy computed in this mhold iteration\n","          acc_val.append(acc_mhold)\n","          # print(f'{user} reached accuracy of {acc_mhold} for tau_u={tau}, iteration {mhold_n}')\n","\n","        # Computing the maximum validation score considering the iterations of mhold\n","        acc_val_max = max(acc_val)\n","        i_acc_val_max = acc_val.index(acc_val_max)\n","\n","        # Store the net associated with the maximum validation score\n","        net_fitted_tau.append(net_fitted_val[i_acc_val_max])\n","        # Store the maximum validation score\n","        acc_tau.append(acc_val_max)\n","        # print(f'{user} reached best accuracy for tau_u={tau} is {acc_val_max} from iteration {i_acc_val_max}')\n","\n","      # Computing the maximum validation score considering the maximum score in each tau\n","      acc_tau_max = max(acc_tau)\n","      _, start_acc_tau_max, end_acc_tau_max = find_largest_plateau(acc_tau)\n","      i_acc_tau_max = int(np.ceil((end_acc_tau_max-start_acc_tau_max)/2)+start_acc_tau_max)\n","      # i_acc_tau_max = acc_tau.index(acc_tau_max)\n","\n","      # Store tau and X_cache only in the first ensemble iteration\n","      if ens == 0:\n","        # Store the best tau associated with the maximum validation score\n","        tau_best.append(tau_u[i_acc_tau_max])\n","        print(f'The best tau for {user} is {tau_best[idx_user]}')\n","        # Store the X_cache associated with the best tau\n","        X_val_ens.append(X_tau[i_acc_tau_max])\n","\n","      # Store the net associated with the best tau\n","      net_fitted_best = net_fitted_tau[i_acc_tau_max]\n","\n","\n","    # Predicting in the testing set\n","    # In the first iteration of the ensemble, computes the X\n","    # In subsequent iterations, uses X_cache to speed the process\n","    if ens == 0:\n","      data_test = data_splitted[idx_user]['test']\n","      predictions_new, X_test_cache = predictor_new(data_test,\n","                                                    best_centers,\n","                                                    net_fitted_best,\n","                                                    params,\n","                                                    tau_best[idx_user],\n","                                                    post=post)\n","\n","      X_test_ens.append(X_test_cache)\n","\n","    else:\n","      predictions_new = predictor_ens_new(X_test_ens[idx_user],\n","                                          net_fitted_best,\n","                                          post=post)\n","\n","    # Extracting the labels from the test data\n","    labels = label_extractor(data_test, gestures)\n","\n","    # Computing the test accuracy\n","    acc_user = accuracy_new(predictions_new, labels)\n","\n","    acc_users.append(acc_user)\n","\n","  acc_exp.append(acc_users)\n","  print(f'Accuracy in ensemble {ens} was {np.mean(acc_users)}')"],"metadata":{"id":"_JOtktLEcvJ_","executionInfo":{"status":"ok","timestamp":1719417354842,"user_tz":180,"elapsed":3648346,"user":{"displayName":"Gabriel Chaves","userId":"05552850817445874347"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1fdcd47-7c14-4167-8e84-93123f5be24a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The best tau for alejandroFlores is 11.0\n","The best tau for alexandraApellido is 15.0\n","The best tau for andresGuerra is 15.0\n","The best tau for andresJaramillo is 16.0\n","The best tau for cristhianMotoche is 15.0\n","The best tau for dianitaCherrez is 13.0\n","The best tau for homeroApellido is 17.0\n","The best tau for jonathanZea is 15.0\n","The best tau for juanYuquilema is 18.0\n","The best tau for santiagoJaramillo is 17.0\n","Accuracy in ensemble 0 was 0.9824999999999999\n","Accuracy in ensemble 1 was 0.9666666666666666\n","Accuracy in ensemble 2 was 0.9691666666666666\n","Accuracy in ensemble 3 was 0.9766666666666668\n","Accuracy in ensemble 4 was 0.9716666666666667\n","Accuracy in ensemble 5 was 0.9641666666666666\n","Accuracy in ensemble 6 was 0.9724999999999999\n","Accuracy in ensemble 7 was 0.9783333333333333\n","Accuracy in ensemble 8 was 0.9724999999999999\n","Accuracy in ensemble 9 was 0.9708333333333334\n","Accuracy in ensemble 10 was 0.9791666666666667\n","Accuracy in ensemble 11 was 0.9691666666666666\n","Accuracy in ensemble 12 was 0.975\n","Accuracy in ensemble 13 was 0.9650000000000001\n","Accuracy in ensemble 14 was 0.9741666666666667\n","Accuracy in ensemble 15 was 0.9658333333333333\n","Accuracy in ensemble 16 was 0.9758333333333333\n","Accuracy in ensemble 17 was 0.9658333333333333\n","Accuracy in ensemble 18 was 0.97\n","Accuracy in ensemble 19 was 0.9766666666666668\n","Accuracy in ensemble 20 was 0.9708333333333334\n","Accuracy in ensemble 21 was 0.9708333333333332\n","Accuracy in ensemble 22 was 0.975\n","Accuracy in ensemble 23 was 0.9800000000000001\n","Accuracy in ensemble 24 was 0.9808333333333333\n","Accuracy in ensemble 25 was 0.9733333333333334\n","Accuracy in ensemble 26 was 0.9716666666666667\n","Accuracy in ensemble 27 was 0.9724999999999999\n","Accuracy in ensemble 28 was 0.9616666666666667\n","Accuracy in ensemble 29 was 0.9733333333333334\n","Accuracy in ensemble 30 was 0.9724999999999999\n","Accuracy in ensemble 31 was 0.9766666666666666\n","Accuracy in ensemble 32 was 0.9775\n","Accuracy in ensemble 33 was 0.975\n","Accuracy in ensemble 34 was 0.9675\n","Accuracy in ensemble 35 was 0.9766666666666668\n","Accuracy in ensemble 36 was 0.9758333333333333\n","Accuracy in ensemble 37 was 0.9716666666666667\n","Accuracy in ensemble 38 was 0.9633333333333333\n","Accuracy in ensemble 39 was 0.9758333333333333\n","Accuracy in ensemble 40 was 0.9758333333333333\n","Accuracy in ensemble 41 was 0.9708333333333332\n","Accuracy in ensemble 42 was 0.9741666666666667\n","Accuracy in ensemble 43 was 0.9758333333333333\n","Accuracy in ensemble 44 was 0.9783333333333333\n","Accuracy in ensemble 45 was 0.9741666666666667\n","Accuracy in ensemble 46 was 0.9691666666666666\n","Accuracy in ensemble 47 was 0.9741666666666667\n","Accuracy in ensemble 48 was 0.97\n","Accuracy in ensemble 49 was 0.975\n","Accuracy in ensemble 50 was 0.9633333333333333\n","Accuracy in ensemble 51 was 0.9691666666666666\n","Accuracy in ensemble 52 was 0.9741666666666667\n","Accuracy in ensemble 53 was 0.9741666666666667\n","Accuracy in ensemble 54 was 0.9641666666666666\n","Accuracy in ensemble 55 was 0.9708333333333332\n","Accuracy in ensemble 56 was 0.975\n","Accuracy in ensemble 57 was 0.9733333333333334\n","Accuracy in ensemble 58 was 0.9766666666666668\n","Accuracy in ensemble 59 was 0.9666666666666666\n","Accuracy in ensemble 60 was 0.9724999999999999\n","Accuracy in ensemble 61 was 0.9783333333333333\n","Accuracy in ensemble 62 was 0.97\n","Accuracy in ensemble 63 was 0.9658333333333333\n","Accuracy in ensemble 64 was 0.9683333333333334\n","Accuracy in ensemble 65 was 0.9724999999999999\n","Accuracy in ensemble 66 was 0.9716666666666667\n","Accuracy in ensemble 67 was 0.9733333333333333\n","Accuracy in ensemble 68 was 0.9708333333333334\n","Accuracy in ensemble 69 was 0.975\n","Accuracy in ensemble 70 was 0.9741666666666667\n","Accuracy in ensemble 71 was 0.9724999999999999\n","Accuracy in ensemble 72 was 0.9758333333333333\n","Accuracy in ensemble 73 was 0.9775\n","Accuracy in ensemble 74 was 0.97\n","Accuracy in ensemble 75 was 0.9658333333333333\n","Accuracy in ensemble 76 was 0.9733333333333334\n","Accuracy in ensemble 77 was 0.9675\n","Accuracy in ensemble 78 was 0.97\n","Accuracy in ensemble 79 was 0.975\n","Accuracy in ensemble 80 was 0.97\n","Accuracy in ensemble 81 was 0.9775\n","Accuracy in ensemble 82 was 0.9650000000000001\n","Accuracy in ensemble 83 was 0.9733333333333334\n","Accuracy in ensemble 84 was 0.9708333333333334\n","Accuracy in ensemble 85 was 0.9716666666666667\n","Accuracy in ensemble 86 was 0.9691666666666666\n","Accuracy in ensemble 87 was 0.9708333333333334\n","Accuracy in ensemble 88 was 0.9633333333333333\n","Accuracy in ensemble 89 was 0.975\n","Accuracy in ensemble 90 was 0.9708333333333334\n","Accuracy in ensemble 91 was 0.9741666666666667\n","Accuracy in ensemble 92 was 0.9800000000000001\n","Accuracy in ensemble 93 was 0.9758333333333333\n","Accuracy in ensemble 94 was 0.9708333333333332\n","Accuracy in ensemble 95 was 0.9658333333333333\n","Accuracy in ensemble 96 was 0.9775\n","Accuracy in ensemble 97 was 0.975\n","Accuracy in ensemble 98 was 0.975\n","Accuracy in ensemble 99 was 0.9724999999999999\n"]}]},{"cell_type":"code","source":["# tau_best = fast\n","for i in range(10):\n","  print(np.mean([acc[i] for acc in acc_exp]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UKPpNIBaS1JU","executionInfo":{"status":"ok","timestamp":1719365074902,"user_tz":180,"elapsed":272,"user":{"displayName":"Gabriel Chaves","userId":"05552850817445874347"}},"outputId":"eac7fd0a-c3b5-4e87-a574-f4fe98f07046"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9916666666666667\n","0.9388333333333333\n","0.9405833333333331\n","0.9954999999999999\n","0.9804166666666666\n","0.9306666666666668\n","0.9329999999999998\n","1.0\n","0.9988333333333334\n","1.0\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"11rCu3B3WUfboRJOYkqj_mNL_RXt_hG37","timestamp":1719421463604},{"file_id":"1qG3-TlM96FcDpbLOCOPhj1IVg-XEkThD","timestamp":1686341892251},{"file_id":"15yZy_VxUM65R0cc0gtd-syVZ2wXULsZY","timestamp":1677968366785}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}